{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "<div style=\"font-weight: bold; font-size: 180%; color: maroon;\">Pylucene links and examples</div>\n",
    "\n",
    "[Pylucene](http://lucene.apache.org/pylucene/) is a Python wrapper around [Lucene](http://lucene.apache.org/) an open source Java-based indexing and search technology, as well as spellchecking, hit highlighting and advanced analysis/tokenization capabilities.\n",
    "\n",
    "Pylucene provides a Python extension that allows launching a Lucene process and interoperating with it.\n",
    "\n",
    "Pylucene exposes the Lucene API with essentially the same namespace as in the original Java API (hence, most of the [Lucene API](http://lucene.apache.org/core/7_5_0/) can be used to find Pylucene equivalents). Note however that the Lucene native API is Java-based, and hence a bit un-Pythonic. Pylucene contains a number of adapters to make some of the interfaces more palatable for Python processing, but the API feels a bit awkward at times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Importing `lucene` will bring into the Python context all the lucene namespace; from then all lucene modules can be imported (included the support Java modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lucene.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check all the Lucene packages included in this distribution of Pylucene\n",
    "for p in sorted(lucene.CLASSPATH.split(':')):\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first operation is **always** to initialize the lucene backend. This only needs to be done once for each running Python process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "if not lucene.getVMEnv():\n",
    "    lucene.initVM(vmargs=['-Djava.awt.headless=true'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "Let's test a few Lucene components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strings = (\n",
    "    'La lluvia en Sevilla es una pura maravilla',\n",
    "    'En un lugar de La Mancha, de cuyo nombre no quiero acordarme',\n",
    "    u'Con diez cañones por banda, viento en popa a toda vela' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An auxiliary function used in the tokenizer and analyzer examples\n",
    "\n",
    "from org.apache.lucene.analysis.tokenattributes import CharTermAttribute\n",
    "\n",
    "def fetch_terms(obj):\n",
    "    '''fetch all terms from a token list object, as strings'''\n",
    "    termAtt = obj.getAttribute(CharTermAttribute.class_)\n",
    "    try:\n",
    "        obj.clearAttributes()\n",
    "        obj.reset()\n",
    "        while obj.incrementToken():\n",
    "            yield termAtt.toString() \n",
    "    finally:\n",
    "        obj.end()\n",
    "        obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lucene import JArray_char, JArray\n",
    "\n",
    "from org.tartarus.snowball.ext import SpanishStemmer, EnglishStemmer\n",
    "\n",
    "def stem(stemmer, word):\n",
    "    # Add the word\n",
    "    stemmer.setCurrent(JArray_char(word), len(word))\n",
    "    # Fire stemming\n",
    "    stemmer.stem()\n",
    "    # Fetch the output (buffer & size)\n",
    "    result = stemmer.getCurrentBuffer()\n",
    "    l = stemmer.getCurrentBufferLength()\n",
    "    return ''.join(result)[0:l]    \n",
    "\n",
    "st = SpanishStemmer()\n",
    "for w in (u'haciendo', u'lunes', u'vino', u'lápiz'):\n",
    "    print( w, '->', stem(st, w))\n",
    "\n",
    "st = EnglishStemmer()\n",
    "for w in (u'making', u'Monday', u'came', u'pencil'):\n",
    "    print( w, '->', stem(st, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "* [StandardTokenizer](https://lucene.apache.org/core/8_1_1/core/org/apache/lucene/analysis/standard/StandardTokenizer.html) A grammar-based tokenizer constructed with JFlex. This class implements the Word Break rules from the Unicode Text Segmentation algorithm, as specified in [Unicode Standard Annex #29. ](http://unicode.org/reports/tr29/)\n",
    "* [LetterTokenizer](https://lucene.apache.org/core/8_1_1/analyzers-common/org/apache/lucene/analysis/core/LetterTokenizer.html). A tokenizer that divides text at non-letters. That's to say, it defines tokens as maximal strings of adjacent letters, as defined by java.lang.Character.isLetter() predicate. \n",
    "* [NGramTokenizer](https://lucene.apache.org/core/8_1_1/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenizer.html) Tokenizes the input into n-grams of the given size(s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from java.io import StringReader\n",
    "\n",
    "def tokenize( tk, data ):\n",
    "    '''Send a string to a tokenizer and get back the token list'''\n",
    "    tk.setReader( StringReader(data) )\n",
    "    return list(fetch_terms(tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from org.apache.lucene.analysis.standard import StandardTokenizer\n",
    "from org.apache.lucene.analysis.core import LetterTokenizer\n",
    "from org.apache.lucene.analysis.ngram import NGramTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = (StandardTokenizer(), LetterTokenizer(), NGramTokenizer(4, 4))\n",
    "\n",
    "for n, t in enumerate(tokenizers):\n",
    "    print( \"\\n{} -----------\".format(n+1), str(t) )\n",
    "    for s in test_strings:\n",
    "        print( \"\\n\", tokenize(t,s) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzer\n",
    "\n",
    "* [KeywordAnalyzer](https://lucene.apache.org/core/7_5_0/analyzers-common/org/apache/lucene/analysis/core/KeywordAnalyzer.html): \"Tokenizes\" the entire stream as a single token. This is useful for data like zip codes, ids, and some product names.\n",
    "* [SimpleAnalyzer](https://lucene.apache.org/core/7_5_0/analyzers-common/org/apache/lucene/analysis/core/SimpleAnalyzer.html): An Analyzer that filters LetterTokenizer with LowerCaseFilter\n",
    "* [SpanishAnalyzer](https://lucene.apache.org/core/7_5_0/analyzers-common/index.html?org/apache/lucene/analysis/core/SimpleAnalyzer.html):  built from an StandardTokenizer filtered with StandardFilter, LowerCaseFilter, StopFilter, SetKeywordMarkerFilter if a stem exclusion set is provided and SpanishLightStemFilter.\n",
    "* [ShingleAnalyzerWrapper](https://lucene.apache.org/core/7_5_0/analyzers-common/index.html?org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapper.html): A ShingleAnalyzerWrapper wraps a ShingleFilter around another Analyzer. A shingle is another name for a token based n-gram. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from java.io import StringReader\n",
    "    \n",
    "def analyze(anal, data):\n",
    "    '''Send a string to an analizer and get back the analyzed term list'''\n",
    "    ts = anal.tokenStream( \"dummy\", StringReader(data) )\n",
    "    return list(fetch_terms(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from org.apache.lucene.analysis.core import KeywordAnalyzer, SimpleAnalyzer\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.analysis.es import SpanishAnalyzer\n",
    "from org.apache.lucene.analysis.shingle import ShingleAnalyzerWrapper\n",
    "\n",
    "analyzers = ( KeywordAnalyzer(),\n",
    "              SimpleAnalyzer(),\n",
    "              SpanishAnalyzer(),\n",
    "              ShingleAnalyzerWrapper( SimpleAnalyzer(), 2, 3 ),\n",
    "              ShingleAnalyzerWrapper( SpanishAnalyzer(), 2, 3 ),\n",
    "            )\n",
    "\n",
    "for n, a in enumerate(analyzers):\n",
    "    print( \"\\n {} ----------- {}\".format(n+1, a) )\n",
    "    for s in test_strings:\n",
    "        print( \"\\n\", analyze(a,s) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
