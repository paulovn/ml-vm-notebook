{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 Spark essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make it Python2 & Python3 compatible\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "if sys.version[0] == 3:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook deployment includes Spark automatically within each Python notebook kernel. This means that, upon kernel instantiation, there is an [SparkContext](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext) object called `sc` immediatelly available in the Notebook, as in a PySpark shell. Let's take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can inspect some of the SparkContext properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark version we are using\n",
    "print( sc.version )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the application we are running\n",
    "print(sc.appName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some configuration variables\n",
    "print( sc.defaultParallelism )\n",
    "print( sc.defaultMinPartitions )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Username running all Spark processes\n",
    "# --> Note this is a method, not a property\n",
    "print( sc.sparkUser() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the SparkContext configuration\n",
    "print( sc._conf.toDebugString() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to get similar information\n",
    "from pyspark import SparkConf, SparkContext\n",
    "SparkConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark execution modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the Spark configuration this kernel is running under, by using the above configuration data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( sc._conf.toDebugString() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... this includes the execution mode for Spark. The default mode is *local*, i.e. all Spark processes run locally in the launched Virtual Machine. This is fine for developing and testing with small datasets.\n",
    "\n",
    "But to run Spark applications on bigger datasets, they must be executed in a remote cluster. This deployment comes with configuration modes for that, which require:\n",
    "* network adjustments to make the VM \"visible\" from the cluster: the virtual machine must be started in _bridged_ mode (the default *Vagrantfile* already contains code for doingso, but it must be uncommented)\n",
    "* configuring the addresses for the cluster. This is done within the VM by using the `spark-notebook` script, such as\n",
    "      sudo service spark-notebook set-addr <master-ip> <namenode-ip> <historyserver-ip>\n",
    "* activating the desired mode, by executing\n",
    "      sudo service spark-notebook set-mode (local | standalone | yarn)\n",
    "\n",
    "These operations can also be performed outside the VM by telling vagrant to relay them, e.g.\n",
    "\n",
    "    vagrant ssh -c \"sudo service spark-notebook set-mode local\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A trivial test\n",
    "\n",
    "Let's do a trivial operation that creates an RDD and executes an action on it. So that we can test that the kernel is capable of launching executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "l = sc.range(10000)\n",
    "print( l.reduce(add) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (Py3)",
   "language": "",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
