{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Spark basics\n",
    "\n",
    "This notebook is a very simple check with [Spark R](https://spark.apache.org/docs/latest/sparkr.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SparkR package. \n",
    "# It will likely show a few warnings about functions that the package overrides\n",
    "library(SparkR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the IRkernel we do not have an automatically created Spark Session, as in Python & Scala. \n",
    "# We need to initialize the kernel to fetch one. That takes a few moments.\n",
    "sc <- sparkR.session( \"local[*]\" );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do something to prove it works\n",
    "\n",
    "# Load one of the standard datasets that come pre-packaged with R\n",
    "data(iris)\n",
    "\n",
    "# Turn the dataset into an SparkR DataFrame\n",
    "df <- as.DataFrame(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect it\n",
    "head( filter(df, df$Petal_Width > 0.2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
