{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark RDD basic manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD creation\n",
    "\n",
    "We create a simple RDD by paralellizing a collection local to the driver (usually they would be created by fetching from external sources or reading from files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All numbers from 0 to 1000. Split in 4 partitions\n",
    "numbers = sc.parallelize( range(0,1001), 4 )\n",
    "\n",
    "print numbers.getNumPartitions()\n",
    "print numbers.count()\n",
    "print numbers.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations & actions\n",
    "\n",
    "We want to compute $\\sum_{i=0}^{499} cos(2i+1) $ using the `numbers` RDD we have just created. We are going to make it a bit convoluted, just to be able to chain transformations; in practice we could do it in a more direct way.\n",
    "\n",
    "First we start by taking only the odd numbers: the list of odd numbers from 0 to 1000 is the same as the list of $(2i+1)$ when $ i \\in [0,499] $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transformation: take only the odd numbers\n",
    "odd = numbers.filter( lambda x : x % 2 )\n",
    "\n",
    "odd.take(10)  # action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the cosine of each number. We could use a `map` with\n",
    "```Python\n",
    "  lambda x : cos(x)\n",
    "```\n",
    "but in this case, since it's just calling a function, we use the function directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transformation: compute the cosine of each number\n",
    "from math import cos\n",
    "odd_cosine = odd.map( cos )\n",
    "\n",
    "odd_cosine.take(10) # action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we sum all values. Again, we could use a lambda function such as\n",
    "```Python\n",
    "  lambda a,b : a+b\n",
    "```\n",
    "but Python [already defines](https://docs.python.org/2/library/operator.html#mapping-operators-to-functions) the \"sum\" function for us, so we just use it.\n",
    "\n",
    "Note this is an action, therefore is the one that triggers the stage computation; the previous transformations didn't produce results (that's in theory, in practive since we executed `take`, we forced realization of the operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Action: sum all values\n",
    "from operator import add\n",
    "result = odd_cosine.reduce( add )\n",
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The many forms of mapping\n",
    "## map vs. flatMap\n",
    "\n",
    "We create a small RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = sc.parallelize( xrange(20), 4 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do a classic map+reduce to sum its squared values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b1 = a.map( lambda x : x*x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "result1 = b1.reduce( add )\n",
    "\n",
    "print result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with  [`flatMap`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap). First let's do it wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b2 = a.flatMap( lambda x : x*x )\n",
    "\n",
    "# This will trigger an error\n",
    "b2.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it right: `flatMap` must produce a list. Even if it's a list of 1 element (or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure flatMap returns a list, even if it's a list of 1\n",
    "b2 = a.flatMap( lambda x : [x*x] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result2 = b2.reduce( add )\n",
    "\n",
    "print result2\n",
    "result2 == result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, why should we use flatMap? Because we can create _several_ rows (including zero) out of each input RDD rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b2b = a.flatMap( lambda x : [x, x*x] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b2b.take(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## map vs. mapPartitions\n",
    "\n",
    "We repeat the same operation as above, but using [`mapPartitions`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mapPartitions). This time is different: our function will not receive an element, but a whole partition (actually an iterator over its elements). We must iterate over it and return another **iterator** over the result of our computation.\n",
    "\n",
    "Admittedly, to use `mapPartitions` for this operation does not make much sense. But in general it might be handy to have access in our function to all the elements in a partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In Python, the easiest way of returning an iterator is by creating \n",
    "# a generator function via yield\n",
    "def mapper( it ):\n",
    "    for n in it:\n",
    "        yield n*n\n",
    "\n",
    "# Now we have the function, let's use it\n",
    "b3 = a.mapPartitions( mapper )\n",
    "result3 = b3.reduce( add )\n",
    "print result3\n",
    "result3 == result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mapPartitions vs. mapPartitionsWithIndex\n",
    "\n",
    "For a final twist, [`mapPartitionsWithIndex`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mapPartitionsWithIndex) works the same as `mapPartitions`, but our function will receive two arguments: the iterator over the elements of the partition, as before, _and_ the index of the partition, i.e. an integer in $[0,numPartitions)$. So we can know which partition we are in when processing its elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In Python, the easiest way of returning an iterator is by creating \n",
    "# a generator function via yield\n",
    "def mapper( partitionIndex, it ):\n",
    "    for n in it:\n",
    "        yield n*n\n",
    "\n",
    "# Now we have the function, let's use it\n",
    "b4 = a.mapPartitionsWithIndex( mapper )\n",
    "result4 = b4.reduce( add )\n",
    "print result4\n",
    "result4 == result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... though in this case we have not used the index, it might be useful for certain tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
